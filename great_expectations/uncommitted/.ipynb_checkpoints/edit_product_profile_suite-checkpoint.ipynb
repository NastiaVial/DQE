{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241c3688",
   "metadata": {},
   "source": [
    "# Initialize a new Expectation Suite by profiling a batch of your data.\n",
    "This process helps you avoid writing lots of boilerplate when authoring suites by allowing you to select columns and other factors that you care about and letting a profiler write some candidate expectations for you to adjust.\n",
    "\n",
    "**Expectation Suite Name**: `product_profile_suite`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf64f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28T12:44:00+0200 - INFO - Great Expectations logging enabled at 20 level by JupyterUX module.\n",
      "2023-04-28T12:44:00+0200 - INFO - FileDataContext loading fluent config\n",
      "2023-04-28T12:44:00+0200 - INFO - GxConfig.parse_yaml() failed with errors - [{'loc': ('fluent_datasources',), 'msg': 'field required', 'type': 'value_error.missing'}]\n",
      "2023-04-28T12:44:00+0200 - INFO - GxConfig.parse_yaml() returning empty `fluent_datasources`\n",
      "2023-04-28T12:44:00+0200 - INFO - Loading 'datasources' ->\n",
      "{}\n",
      "2023-04-28T12:44:00+0200 - INFO - Loaded 'datasources' ->\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1a97a77d5f41148c4f7870a4042482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: \"ProductID\", \"Name\", \"ProductNumber\", \"MakeFlag\", \"FinishedGoodsFlag\", \"Color\", \"SafetyStockLevel\", \"ReorderPoint\", \"StandardCost\", \"ListPrice\", \"Size\", \"SizeUnitMeasureCode\", \"WeightUnitMeasureCode\", \"Weight\", \"DaysToManufacture\", \"ProductLine\", \"Class\", \"Style\", \"ProductSubcategoryID\", \"ProductModelID\", \"SellStartDate\", \"SellEndDate\", \"DiscontinuedDate\", \"rowguid\", \"ModifiedDate\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e69cdf26c564de385d48e9d7e4c1371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28T12:44:01+0200 - WARNING - Exceptions\n",
      "{('table.head', 'batch_id=198755f4192e46c59cff041f7722cd8b', '04166707abe073177c1dd922d3584468'): {'metric_configuration': {\n",
      "  \"metric_name\": \"table.head\",\n",
      "  \"metric_domain_kwargs\": {\n",
      "    \"batch_id\": \"198755f4192e46c59cff041f7722cd8b\"\n",
      "  },\n",
      "  \"metric_domain_kwargs_id\": \"batch_id=198755f4192e46c59cff041f7722cd8b\",\n",
      "  \"metric_value_kwargs\": {\n",
      "    \"n_rows\": 5,\n",
      "    \"fetch_all\": false\n",
      "  },\n",
      "  \"metric_value_kwargs_id\": \"04166707abe073177c1dd922d3584468\",\n",
      "  \"id\": [\n",
      "    \"table.head\",\n",
      "    \"batch_id=198755f4192e46c59cff041f7722cd8b\",\n",
      "    \"04166707abe073177c1dd922d3584468\"\n",
      "  ]\n",
      "}, 'num_failures': 3, 'exception_info': {{'exception_traceback': 'Traceback (most recent call last):\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\", line 555, in _process_direct_and_bundled_metric_computation_configurations\\n    ] = metric_computation_configuration.metric_fn(  # type: ignore[misc] # F not callable\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\metric_provider.py\", line 50, in inner_func\\n    return metric_fn(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\great_expectations\\\\expectations\\\\metrics\\\\table_metrics\\\\table_head.py\", line 113, in _sqlalchemy\\n    df_chunk_iterator = pd.read_sql_table(\\n                        ^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\sql.py\", line 338, in read_sql_table\\n    table = pandas_sql.read_table(\\n            ^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\sql.py\", line 1625, in read_table\\n    self.meta.reflect(bind=self.con, only=[table_name])\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\sqlalchemy\\\\sql\\\\schema.py\", line 4889, in reflect\\n    raise exc.InvalidRequestError(\\nsqlalchemy.exc.InvalidRequestError: Could not reflect: requested table(s) not available in Engine(mssql+pyodbc://nastia:***@EPPLGDAW00A5\\\\SQLEXPRESS1:1433/AdventureWorks2016?autocommit=true&charset=utf&driver=ODBC+Driver+17+for+SQL+Server): (#ge_temp_5772e3ad)\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\great_expectations\\\\validator\\\\validation_graph.py\", line 276, in _resolve\\n    self._execution_engine.resolve_metrics(\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\", line 290, in resolve_metrics\\n    return self._process_direct_and_bundled_metric_computation_configurations(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"C:\\\\Users\\\\Anastasiya_Vial\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\great_expectations\\\\execution_engine\\\\execution_engine.py\", line 559, in _process_direct_and_bundled_metric_computation_configurations\\n    raise gx_exceptions.MetricResolutionError(\\ngreat_expectations.exceptions.exceptions.MetricResolutionError: Could not reflect: requested table(s) not available in Engine(mssql+pyodbc://nastia:***@EPPLGDAW00A5\\\\SQLEXPRESS1:1433/AdventureWorks2016?autocommit=true&charset=utf&driver=ODBC+Driver+17+for+SQL+Server): (#ge_temp_5772e3ad)\\n', 'exception_message': 'Could not reflect: requested table(s) not available in Engine(mssql+pyodbc://nastia:***@EPPLGDAW00A5\\\\SQLEXPRESS1:1433/AdventureWorks2016?autocommit=true&charset=utf&driver=ODBC+Driver+17+for+SQL+Server): (#ge_temp_5772e3ad)', 'raised_exception': True}}}}\n",
      "occurred while resolving metrics.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('table.head', 'batch_id=198755f4192e46c59cff041f7722cd8b', '04166707abe073177c1dd922d3584468')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mcolumns()]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(column_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\great_expectations\\validator\\validator.py:425\u001b[0m, in \u001b[0;36mValidator.head\u001b[1;34m(self, n_rows, domain_kwargs, fetch_all)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;129m@public_api\u001b[39m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhead\u001b[39m(\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m     fetch_all: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    414\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    415\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the first several rows or records from a Batch of data.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m        A Pandas DataFrame containing the records' data.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics_calculator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdomain_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_all\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\great_expectations\\validator\\metrics_calculator.py:86\u001b[0m, in \u001b[0;36mMetricsCalculator.head\u001b[1;34m(self, n_rows, domain_kwargs, fetch_all)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m domain_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     domain_kwargs[\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mbatch_manager\u001b[38;5;241m.\u001b[39mactive_batch_id\n\u001b[1;32m---> 86\u001b[0m df: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetricConfiguration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable.head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_domain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdomain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_value_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_rows\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfetch_all\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\great_expectations\\validator\\metrics_calculator.py:104\u001b[0m, in \u001b[0;36mMetricsCalculator.get_metric\u001b[1;34m(self, metric)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metric\u001b[39m(\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    101\u001b[0m     metric: MetricConfiguration,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the value of the requested metric.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[metric\u001b[38;5;241m.\u001b[39mmetric_name]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\great_expectations\\validator\\metrics_calculator.py:126\u001b[0m, in \u001b[0;36mMetricsCalculator.get_metrics\u001b[1;34m(self, metrics)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    metrics: Dictionary of desired metrics to be resolved; metric_name is key and MetricConfiguration is value.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Return Dictionary with requested metrics resolved, with metric_name as key and computed metric as value.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m resolved_metrics: Dict[\n\u001b[0;32m    120\u001b[0m     Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], MetricValue\n\u001b[0;32m    121\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     min_graph_edges_pbar_enable\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    125\u001b[0m )\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetric_configuration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\great_expectations\\validator\\metrics_calculator.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    metrics: Dictionary of desired metrics to be resolved; metric_name is key and MetricConfiguration is value.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Return Dictionary with requested metrics resolved, with metric_name as key and computed metric as value.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m resolved_metrics: Dict[\n\u001b[0;32m    120\u001b[0m     Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], MetricValue\n\u001b[0;32m    121\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     min_graph_edges_pbar_enable\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m--> 127\u001b[0m     metric_configuration\u001b[38;5;241m.\u001b[39mmetric_name: \u001b[43mresolved_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric_configuration \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    129\u001b[0m }\n",
      "\u001b[1;31mKeyError\u001b[0m: ('table.head', 'batch_id=198755f4192e46c59cff041f7722cd8b', '04166707abe073177c1dd922d3584468')"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import great_expectations as gx\n",
    "import great_expectations.jupyter_ux\n",
    "from great_expectations.core.batch import BatchRequest\n",
    "from great_expectations.checkpoint import SimpleCheckpoint\n",
    "from great_expectations.exceptions import DataContextError\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "batch_request = {'datasource_name': 'my_datasource', 'data_connector_name': 'default_configured_data_connector_name', 'data_asset_name': 'Product', 'limit': 1000}\n",
    "\n",
    "expectation_suite_name = \"product_profile_suite\"\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=BatchRequest(**batch_request),\n",
    "    expectation_suite_name=expectation_suite_name\n",
    ")\n",
    "column_names = [f'\"{column_name}\"' for column_name in validator.columns()]\n",
    "print(f\"Columns: {', '.join(column_names)}.\")\n",
    "validator.head(n_rows=5, fetch_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e7cff",
   "metadata": {},
   "source": [
    "# Select columns\n",
    "\n",
    "Select the columns on which you would like to set expectations and those which you would like to ignore.\n",
    "\n",
    "Great Expectations will choose which expectations might make sense for a column based on the **data type** and **cardinality** of the data in each selected column.\n",
    "\n",
    "Simply comment out columns that are important and should be included. You can select multiple lines and use a Jupyter\n",
    "keyboard shortcut to toggle each line: **Linux/Windows**:\n",
    "`Ctrl-/`, **macOS**: `Cmd-/`\n",
    "\n",
    "Other directives are shown (commented out) as examples of the depth of control possible (see documentation for details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_column_names = [\n",
    "    \"ProductID\",\n",
    "    \"Name\",\n",
    "    \"ProductNumber\",\n",
    "    \"MakeFlag\",\n",
    "    \"FinishedGoodsFlag\",\n",
    "    \"Color\",\n",
    "    \"SafetyStockLevel\",\n",
    "    \"ReorderPoint\",\n",
    "    \"StandardCost\",\n",
    "    \"ListPrice\",\n",
    "    \"Size\",\n",
    "    \"SizeUnitMeasureCode\",\n",
    "    \"WeightUnitMeasureCode\",\n",
    "    \"Weight\",\n",
    "    \"DaysToManufacture\",\n",
    "    \"ProductLine\",\n",
    "    \"Class\",\n",
    "    \"Style\",\n",
    "    \"ProductSubcategoryID\",\n",
    "    \"ProductModelID\",\n",
    "    \"SellStartDate\",\n",
    "    \"SellEndDate\",\n",
    "    \"DiscontinuedDate\",\n",
    "    \"rowguid\",\n",
    "    \"ModifiedDate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd0473",
   "metadata": {},
   "source": [
    "# Run the OnboardingDataAssistant\n",
    "\n",
    "The suites generated here are **not meant to be production suites** -- they are **a starting point to build upon**.\n",
    "\n",
    "**To get to a production-grade suite, you will definitely want to [edit this\n",
    "suite](https://docs.greatexpectations.io/docs/guides/expectations/create_expectations_overview#editing-a-saved-expectation-suite)\n",
    "after this initial step gets you started on the path towards what you want.**\n",
    "\n",
    "This is highly configurable depending on your goals.\n",
    "You can ignore columns, specify cardinality of categorical columns, configure semantic types for columns, even adjust thresholds and/or different estimator parameters, etc.\n",
    "You can find more information about OnboardingDataAssistant and other DataAssistant components (please see documentation for the complete set of DataAssistant controls) [how to choose and control the behavior of the DataAssistant tailored to your goals](https://docs.greatexpectations.io/docs/guides/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant).\n",
    "\n",
    "Performance considerations:\n",
    "- Latency: We optimized for an explicit \"question/answer\" design, which means we issue **lots** of queries. Connection latency will impact performance.\n",
    "- Data Volume: Small samples of data will often give you a great starting point for understanding the dataset. Consider configuring a sampled asset and profiling a small number of batches.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2496c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = context.assistants.onboarding.run(\n",
    "    batch_request=batch_request,\n",
    "    exclude_column_names=exclude_column_names,\n",
    ")\n",
    "validator.expectation_suite = result.get_expectation_suite(\n",
    "    expectation_suite_name=expectation_suite_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b4a85",
   "metadata": {},
   "source": [
    "# Save & review your new Expectation Suite\n",
    "\n",
    "Let's save the draft expectation suite as a JSON file in the\n",
    "`great_expectations/expectations` directory of your project and rebuild the Data\n",
    " Docs site to make it easy to review your new suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e128320",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validator.get_expectation_suite(discard_failed_expectations=False))\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "checkpoint_config = {\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"validations\": [\n",
    "        {\n",
    "            \"batch_request\": batch_request,\n",
    "            \"expectation_suite_name\": expectation_suite_name\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "checkpoint = SimpleCheckpoint(\n",
    "    f\"{validator.active_batch_definition.data_asset_name}_{expectation_suite_name}\",\n",
    "    context,\n",
    "    **checkpoint_config\n",
    ")\n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "context.build_data_docs()\n",
    "\n",
    "validation_result_identifier = checkpoint_result.list_validation_result_identifiers()[0]\n",
    "context.open_data_docs(resource_identifier=validation_result_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69888202",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "After you review this initial Expectation Suite in Data Docs you\n",
    "should edit this suite to make finer grained adjustments to the expectations.\n",
    "This can be done by running `great_expectations suite edit product_profile_suite`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
